{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Homework \n",
    "\n",
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "For this homework we will be using the FHV 2019-10 data found here. [FHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz)\n",
    "\n",
    "> [!NOTE]  \n",
    "> \n",
    "> Make sure to run [bash script](../../../05-batch/code/download_data.sh) to prepare data before doing homework. We had done for yellow and green for years 2020-2021, when following [Video 5.3.3](https://www.youtube.com/watch?v=CI3P4tAtru4&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=56). But now need the dataset for homework as linked above.\n",
    "\n",
    "```bash \n",
    "./download_data.sh fhv 2019\n",
    "```\n",
    "\n",
    "Actually just need the one file for 2019-10 for this homeowork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/03/03 17:42:33 WARN Utils: Your hostname, Cinders resolves to a loopback address: 127.0.1.1; using 172.17.156.62 instead (on interface eth0)\n",
      "24/03/03 17:42:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/03 17:42:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: \n",
    "\n",
    "### Answer 1: `'3.5.1'`\n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?\n",
    "\n",
    "> [!NOTE]\n",
    "> \n",
    "> To install PySpark follow this [guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_infile = f'../../../data/csv/taxi+_zone_lookup.csv'\n",
    "zones_outpath = f'../../../data/zones/'\n",
    "fhv_infile = f'../../../data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv.gz'\n",
    "fhv_outpath = f'../../../data/pq/fhv/2019/10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(fhv_infile, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID                int64\n",
       "DOlocationID                int64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0           264           264      NaN                 B00009  \n",
       "1           264           264      NaN                 B00013  \n",
       "2           264           264      NaN                 B00014  \n",
       "3           264           264      NaN                 B00014  \n",
       "4           264           264      NaN                 B00014  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "fhv_schema = types.StructType([\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropOff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.DoubleType(), True),\n",
    "    types.StructField(\"Affiliated_base_number\", types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/de-lessons\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv_10 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(fhv_schema) \\\n",
    "    .csv(fhv_infile)\n",
    "    \n",
    "df_fhv_10 \\\n",
    "    .repartition(6) \\\n",
    "    .write.parquet(fhv_outpath, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx 1 ellabelle ellabelle 6670578 Mar  3 17:42 \u001b[0m\u001b[01;32m../../../data/pq/fhv/2019/10/part-00000-736c78e4-b761-4537-b49c-55f9e45c368d-c000.snappy.parquet\u001b[0m\u001b[K*\n",
      "-rwxrwxrwx 1 ellabelle ellabelle 6660082 Mar  3 17:42 \u001b[01;32m../../../data/pq/fhv/2019/10/part-00001-736c78e4-b761-4537-b49c-55f9e45c368d-c000.snappy.parquet\u001b[0m\u001b[K*\n",
      "-rwxrwxrwx 1 ellabelle ellabelle 6665599 Mar  3 17:42 \u001b[01;32m../../../data/pq/fhv/2019/10/part-00002-736c78e4-b761-4537-b49c-55f9e45c368d-c000.snappy.parquet\u001b[0m\u001b[K*\n",
      "-rwxrwxrwx 1 ellabelle ellabelle 6655793 Mar  3 17:42 \u001b[01;32m../../../data/pq/fhv/2019/10/part-00003-736c78e4-b761-4537-b49c-55f9e45c368d-c000.snappy.parquet\u001b[0m\u001b[K*\n",
      "-rwxrwxrwx 1 ellabelle ellabelle 6664355 Mar  3 17:42 \u001b[01;32m../../../data/pq/fhv/2019/10/part-00004-736c78e4-b761-4537-b49c-55f9e45c368d-c000.snappy.parquet\u001b[0m\u001b[K*\n",
      "-rwxrwxrwx 1 ellabelle ellabelle 6667744 Mar  3 17:42 \u001b[01;32m../../../data/pq/fhv/2019/10/part-00005-736c78e4-b761-4537-b49c-55f9e45c368d-c000.snappy.parquet\u001b[0m\u001b[K*\n"
     ]
    }
   ],
   "source": [
    "ls -la ../../../data/pq/fhv/2019/10/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: \n",
    "\n",
    "### Answer 2: `6MB`\n",
    "\n",
    "**FHV October 2019**\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 1MB\n",
    "- 6MB\n",
    "- 25MB\n",
    "- 87MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data \n",
    "- from .parquet instead of .csv, \n",
    "- into a spark dataframe instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(fhv_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\\\n",
    "    .filter(F.to_date(df.pickup_datetime) == '2019-10-15')\\\n",
    "    .count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: \n",
    "\n",
    "### Answer 3: `62,610`\n",
    "\n",
    "**Count records** \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "- 108,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 62,610\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Be aware of columns order when defining schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trips showing first 5 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropOff_datetime|\n",
      "+-------------------+-------------------+\n",
      "|2019-10-01 00:00:00|2019-10-01 00:30:00|\n",
      "|2019-10-01 00:00:00|2019-10-01 00:23:00|\n",
      "|2019-10-01 00:00:05|2019-10-01 00:24:26|\n",
      "|2019-10-01 00:00:12|2019-10-02 00:06:06|\n",
      "|2019-10-01 00:00:12|2019-10-01 00:04:06|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .select('pickup_datetime', 'dropOff_datetime')\\\n",
    "    .orderBy(F.col(\"pickup_datetime\").asc())\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trips showing last 5 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropOff_datetime|\n",
      "+-------------------+-------------------+\n",
      "|2019-10-31 23:59:59|2019-11-01 00:08:39|\n",
      "|2019-10-31 23:59:56|2019-11-01 00:21:34|\n",
      "|2019-10-31 23:59:54|2019-11-01 00:15:22|\n",
      "|2019-10-31 23:59:52|2019-11-01 00:11:09|\n",
      "|2019-10-31 23:59:51|2019-11-01 00:09:32|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .select('pickup_datetime', 'dropOff_datetime')\\\n",
    "    .orderBy(F.col(\"pickup_datetime\").desc())\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [order by, sort pyspark sql syntax](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)\n",
    "2. [unix timestamp diff syntax](https://sparkbyexamples.com/pyspark/pyspark-timestamp-difference-seconds-minutes-hours/)\n",
    "\n",
    "- using built-in `unix_timestamp` function, get the difference of dropoff to pickup\n",
    "- since answer is in seconds, convert to hours\n",
    "- select just the 4 columns\n",
    "- sort by `DiffInHours` column, in descending order\n",
    "- show top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+-----------+\n",
      "|    pickup_datetime|   dropOff_datetime|DiffInSeconds|DiffInHours|\n",
      "+-------------------+-------------------+-------------+-----------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|   2272149000|   631153.0|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|   2272149000|   631153.0|\n",
      "|2019-10-31 23:46:33|2029-11-01 00:13:00|    315620787|    87672.0|\n",
      "|2019-10-01 21:43:42|2027-10-01 21:45:23|    252460901|    70128.0|\n",
      "|2019-10-17 14:00:00|2020-10-18 00:00:00|     31658400|     8794.0|\n",
      "+-------------------+-------------------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .withColumn('DiffInSeconds',F.unix_timestamp(\"dropOff_datetime\") - F.unix_timestamp('pickup_datetime')) \\\n",
    "    .withColumn('DiffInHours',F.round(F.col('DiffInSeconds')/3600))\\\n",
    "    .select('pickup_datetime', 'dropOff_datetime','DiffInSeconds','DiffInHours')\\\n",
    "    .orderBy(F.col(\"DiffInHours\").desc())\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There cannot be any taxi trips where the drop-off date is in the ***future***!! Obviously there are outliers from errorneous data collection or data entry. Drop-off date of `year` 2091 is so obviously wrong for first entry with `2091-10-11` or `2091-10-28`. Most likely a typo of `2019`, getting transposed.\n",
    "\n",
    "As there's no instructions to clean or transform data, I'm going to take the answer as-is. The homework question 4 just states `What is the length of the longest trip in the dataset in hours?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "631152.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using built-in `unix_timestamp` function, get the difference of dropoff to pickup\n",
    "# since above answer is in seconds, convert to hours\n",
    "# get the max and take the first entry\n",
    "df.withColumn('DiffInSeconds',F.unix_timestamp(\"dropOff_datetime\") - F.unix_timestamp('pickup_datetime')) \\\n",
    "  .withColumn('DiffInHours', F.col('DiffInSeconds')/3600)\\\n",
    "  .agg(F.max('DiffInHours'))\\\n",
    "  .first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: \n",
    "\n",
    "### Answer 4: `631,152.50 Hours`\n",
    "\n",
    "**Longest trip for each day** \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "- 631,152.50 Hours\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: \n",
    "\n",
    "### Answer 5: `4040`\n",
    "\n",
    "**User Interface**\n",
    "\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "- 80\n",
    "- 443\n",
    "- 4040\n",
    "- 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones=pd.read_csv(zones_infile, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocationID       int64\n",
       "Borough         object\n",
       "Zone            object\n",
       "service_zone    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_schema = types.StructType([\n",
    "    types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"Borough\", types.StringType(), True),\n",
    "    types.StructField(\"Zone\", types.StringType(), True),\n",
    "    types.StructField(\"service_zone\", types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(zones_schema)\\\n",
    "    .csv(zones_infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+------------+\n",
      "|LocationID|Borough|          Zone|service_zone|\n",
      "+----------+-------+--------------+------------+\n",
      "|         1|    EWR|Newark Airport|         EWR|\n",
      "|         2| Queens|   Jamaica Bay|   Boro Zone|\n",
      "+----------+-------+--------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df\\\n",
    "    .join(df_zones, df.PULocationID == df_zones.LocationID)\\\n",
    "    .where(F.col('Borough') != 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jamaica Bay'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined\\\n",
    "    .groupBy('Zone')\\\n",
    "    .count()\\\n",
    "    .orderBy('count', ascending=True)\\\n",
    "    .first()[0]\n",
    "    # .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: \n",
    "\n",
    "### Answer 6: `Jamaica Bay`\n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>\n",
    "\n",
    "- East Chelsea\n",
    "- Jamaica Bay\n",
    "- Union Sq\n",
    "- Crown Heights North\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('view_fhv')\n",
    "df_zones.createOrReplaceTempView('view_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_joined = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_zone.Zone AS pickup_zone,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    view_fhv as fhv\n",
    "INNER JOIN view_zones as pickup_zone\n",
    "ON fhv.PULocationID = pickup_zone.LocationID\n",
    "WHERE\n",
    "    pickup_zone.Borough != 'Unknown'\n",
    "GROUP BY\n",
    "    pickup_zone\n",
    "ORDER BY number_records\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|         pickup_zone|number_records|\n",
      "+--------------------+--------------+\n",
      "|         Jamaica Bay|             1|\n",
      "|Governor's Island...|             2|\n",
      "| Green-Wood Cemetery|             5|\n",
      "|       Broad Channel|             8|\n",
      "|     Highbridge Park|            14|\n",
      "+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
