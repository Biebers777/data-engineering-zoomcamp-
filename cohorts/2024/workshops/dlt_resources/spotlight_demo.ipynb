{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abaa769cf924bf5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "`dlt` is an open-source library that you can add to your Python scripts to load data from various and often messy data sources into well-structured, live datasets.\n",
    "\n",
    "How it works?\n",
    "\n",
    "`dlt` extracts data from a source, inspects its structure to generate a schema, organizes, normalizes and verifies the data, and loads the data into a destination, such as a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d91f5b4f22a285",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![img](../dlt_resources/dlt-high-level.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240f2fb-1909-4ef4-b80e-5b76c1aa77aa",
   "metadata": {},
   "source": [
    "Below, we give you a preview of how you can get data from APIs, files, Python objects or pandas dataframes and move it into a local or remote database, data lake or a vector data store. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0bcb3-f6f3-40b4-a8f5-9d1c0bd7084a",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Official releases of dlt can be installed from [PyPI](https://pypi.org/project/dlt/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8088e93-0f5d-4975-a670-29e1cc58607a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:08:35.171038620Z",
     "start_time": "2023-09-04T09:08:32.710649363Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25f96c-444c-4b56-b1af-0e1075145181",
   "metadata": {},
   "source": [
    "Command above just installs library core, in example below we use `duckdb` as a [destination](https://dlthub.com/docs/dlt-ecosystem/destinations), so let's add it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5059c-d0e6-49a1-9e41-998d14022baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:08:58.838489545Z",
     "start_time": "2023-09-04T09:08:55.336391284Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q \"dlt[duckdb]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34b265-3ae7-443c-985c-28740068c9e6",
   "metadata": {},
   "source": [
    "> Use clean virtual environment for your experiments! Here are [detailed instructions](https://dlthub.com/docs/reference/installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143f5c7-e234-4dbc-b62c-c974bfc1c815",
   "metadata": {},
   "source": [
    "## Quick start\n",
    "\n",
    "Let's load a list of Python objects (dicts) into `duckdb` database and inspect the created dataset.\n",
    "\n",
    "> We gonna use `full_refresh` for our test examples. If you create a new pipeline script you will be experimenting a lot. If you want that each time the pipeline resets its state and loads data to a new dataset, set the full_refresh argument of the dlt.pipeline method to True. Each time the pipeline is created, dlt adds datetime-based suffix to the dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fbbd7b-f688-416b-9efc-0f5cf12b224f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:10:47.221276167Z",
     "start_time": "2023-09-04T09:10:46.744997133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline quick_start load step completed in 0.41 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata_20240210113244\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/quick_start.duckdb location to store data\n",
      "Load package 1707564764.3130832 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "data = [\n",
    "\t{'id': 1, 'name': 'Alice'},\n",
    "\t{'id': 2, 'name': 'Bob'}\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='quick_start',\n",
    "\tdestination='duckdb',\n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=True, \n",
    ")\n",
    "load_info = pipeline.run(data, table_name=\"users\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24e9e0-8903-40e0-9b45-43ac5333e7c4",
   "metadata": {},
   "source": [
    "### Now explore your data! \n",
    "\n",
    "To see the schema of your created database, run Streamlit command:\n",
    "\n",
    "```python\n",
    " dlt pipeline <pipeline_name> show\n",
    "```\n",
    "[This command](https://dlthub.com/docs/reference/command-line-interface#show-tables-and-data-in-the-destination) generates and launches a simple Streamlit app that you can use to inspect the schemas and data in the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706abe8-e820-4bf5-b9dc-20c4562f782e",
   "metadata": {},
   "source": [
    "To use `streamlit`, install it first.\n",
    "\n",
    "For example above pipeline name is “quick_start”, so run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92682ee3-6cc9-411f-b926-abb362995bd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T17:58:19.054419846Z",
     "start_time": "2023-09-02T17:58:17.780008365Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q streamlit pandas==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9627343-508c-4f82-90bb-077f7dca392b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:13:39.254140397Z",
     "start_time": "2023-09-04T09:11:39.863163280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pipeline \u001b[1mquick_start\u001b[0m in \u001b[1m/home/ellabelle/.dlt/pipelines\u001b[0m\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://172.17.156.62:8501\n",
      "\n",
      "^C\n",
      "  Stopping...\n"
     ]
    }
   ],
   "source": [
    "!dlt pipeline quick_start show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91fe343542c621",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a81884-9abe-4694-9c65-61e03a748cdc",
   "metadata": {},
   "source": [
    "## Load data from variety of sources\n",
    "\n",
    "Use dlt to load practically any data you deal with in your Python script into a dataset. \n",
    "\n",
    "The library will create/update tables, infer data types and deal with nested data automatically:\n",
    "- list of dicts\n",
    "- json\n",
    "- csv\n",
    "- API\n",
    "- database\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65b183-14bd-40af-a105-ca75c02d31c9",
   "metadata": {},
   "source": [
    "### from JSON\n",
    "\n",
    "When creating a schema during normalization, dlt recursively unpacks this nested structure into relational tables, creating and linking [children and parent tables](https://dlthub.com/docs/dlt-ecosystem/visualizations/understanding-the-tables#child-and-parent-tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03def52-2831-4208-a0f2-340c010afa31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:14:43.865760881Z",
     "start_time": "2023-09-04T09:14:43.791750796Z"
    }
   },
   "outputs": [],
   "source": [
    "# create test json file\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"test.json\", 'w') as file:\n",
    "    data = {\n",
    "        'id': 1, \n",
    "        'name': 'Alice', \n",
    "        'job': {\n",
    "            \"company\": \"ScaleVector\",\n",
    "            \"title\": \"Data Scientist\",\n",
    "        },\n",
    "        'children': [\n",
    "            {\n",
    "                'id': 1, \n",
    "                'name': 'Eve'\n",
    "            },\n",
    "            {\n",
    "                'id': 2, \n",
    "                'name': 'Wendy'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    json.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eba4a6-806f-4e69-966b-6804196bc5fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:14:46.335517704Z",
     "start_time": "2023-09-04T09:14:45.714030334Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load test json to duckdb database\n",
    "\n",
    "import json\n",
    "import dlt\n",
    "\n",
    "\n",
    "with open(\"test.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='from_json',\n",
    "\tdestination='duckdb', \n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=True,\n",
    ")\n",
    "# dlt works with lists of dicts, so wrap data to the list\n",
    "load_info = pipeline.run([data], table_name=\"json_data\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba66dc7-1607-4eaf-91f3-3a7faaf4e845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:15:46.956241708Z",
     "start_time": "2023-09-04T09:15:46.877783301Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "display(conn.sql(\"DESCRIBE\"))\n",
    "data_table = conn.sql(\"SELECT * FROM json_data\").df()\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e485b-9230-421e-8715-04500bbfce35",
   "metadata": {},
   "source": [
    "### from API\n",
    "\n",
    "Below we load 100 most recent issues from our [own dlt repository](https://github.com/dlt-hub/dlt) into \"issues\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94608d3-ecab-462e-970a-5848de352833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:18:01.252793256Z",
     "start_time": "2023-09-04T09:17:59.638279822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline from_api load step completed in 0.50 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata_20240210113549\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/from_api.duckdb location to store data\n",
      "Load package 1707564949.1378694 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import requests\n",
    "\n",
    "\n",
    "# url to request dlt-hub/dlt issues\n",
    "url = \"https://api.github.com/repos/dlt-hub/dlt/issues\"\n",
    "# make the request and check if succeeded\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='from_api',\n",
    "\tdestination='duckdb', \n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=True,\n",
    ")\n",
    "load_info = pipeline.run(response.json(), table_name=\"issues\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8acf3b3c-8657-4e0b-a63e-f21d86205389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:18:04.041808808Z",
     "start_time": "2023-09-04T09:18:03.950112708Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬──────────────────────┬─────────────────────┬──────────────────────┬───────────────────────────┬───────────┐\n",
       "│ database │        schema        │        name         │     column_names     │       column_types        │ temporary │\n",
       "│ varchar  │       varchar        │       varchar       │      varchar[]       │         varchar[]         │  boolean  │\n",
       "├──────────┼──────────────────────┼─────────────────────┼──────────────────────┼───────────────────────────┼───────────┤\n",
       "│ from_api │ mydata_20240210113…  │ _dlt_loads          │ [load_id, schema_n…  │ [VARCHAR, VARCHAR, BIGI…  │ false     │\n",
       "│ from_api │ mydata_20240210113…  │ _dlt_pipeline_state │ [version, engine_v…  │ [BIGINT, BIGINT, VARCHA…  │ false     │\n",
       "│ from_api │ mydata_20240210113…  │ _dlt_version        │ [version, engine_v…  │ [BIGINT, BIGINT, TIMEST…  │ false     │\n",
       "│ from_api │ mydata_20240210113…  │ issues              │ [url, repository_u…  │ [VARCHAR, VARCHAR, VARC…  │ false     │\n",
       "│ from_api │ mydata_20240210113…  │ issues__assignees   │ [login, id, node_i…  │ [VARCHAR, BIGINT, VARCH…  │ false     │\n",
       "│ from_api │ mydata_20240210113…  │ issues__labels      │ [id, node_id, url,…  │ [BIGINT, VARCHAR, VARCH…  │ false     │\n",
       "└──────────┴──────────────────────┴─────────────────────┴──────────────────────┴───────────────────────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_star()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_star()\n",
       "0            30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "display(conn.sql(\"DESCRIBE\"))\n",
    "data_table = conn.sql(\"SELECT COUNT(*) FROM issues\").df()\n",
    "data_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db4731-9484-4863-908e-7891c8c24bf9",
   "metadata": {},
   "source": [
    "## Append or replace your data\n",
    "\n",
    "Run this examples twice and you notice that each time a copy of the data is added to your tables.\n",
    "We call this load mode `append`. It is very useful when i.e. you have a new folder created daily with `json` file logs, and you want to ingest them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3807b69-bf33-4881-b1f3-d8c744e43f04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:19:04.132440944Z",
     "start_time": "2023-09-04T09:19:03.852544871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline append load step completed in 0.17 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/append.duckdb location to store data\n",
      "Load package 1707565088.623157 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "data = [\n",
    "\t{'id': 1, 'name': 'Alice'},\n",
    "\t{'id': 2, 'name': 'Bob'}\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='append',\n",
    "\tdestination='duckdb',\n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=False, \n",
    ")\n",
    "load_info = pipeline.run(data, table_name=\"users\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59eebaf-b418-473f-82d5-09fe7fca67ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:19:04.893998643Z",
     "start_time": "2023-09-04T09:19:04.865751402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>1707565077.6717787</td>\n",
       "      <td>fuMOCw9PM1dGww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>1707565077.6717787</td>\n",
       "      <td>yNjPgpXAk956uw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>1707565088.623157</td>\n",
       "      <td>ODQKJh/RDwqRUg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>1707565088.623157</td>\n",
       "      <td>CZSY1xsmg4hhvg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name        _dlt_load_id         _dlt_id\n",
       "0   1  Alice  1707565077.6717787  fuMOCw9PM1dGww\n",
       "1   2    Bob  1707565077.6717787  yNjPgpXAk956uw\n",
       "2   1  Alice   1707565088.623157  ODQKJh/RDwqRUg\n",
       "3   2    Bob   1707565088.623157  CZSY1xsmg4hhvg"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "data_table = conn.sql(\"SELECT * FROM users\").df()\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd1ed0-3fc8-4129-89a0-de9b62ba01fe",
   "metadata": {},
   "source": [
    "Perhaps this is not what you want to do in the example above.\n",
    "For example, if the CSV file is updated, how we can refresh it in the database?\n",
    "One method is to tell `dlt` to replace the data in existing tables by using `write_disposition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042bc584-b2da-474c-b72f-be7e822aa1ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:19:45.541779640Z",
     "start_time": "2023-09-04T09:19:45.206176576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline replace load step completed in 0.48 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/replace.duckdb location to store data\n",
      "Load package 1707565129.7864115 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "data = [\n",
    "\t{'id': 1, 'name': 'Alice'},\n",
    "\t{'id': 2, 'name': 'Bob'}\n",
    "]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='replace',\n",
    "\tdestination='duckdb',\n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=False, \n",
    ")\n",
    "load_info = pipeline.run(data, table_name=\"users\", write_disposition=\"replace\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d16b9c-a5e9-40d6-87f3-d744de50791e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:19:46.367758599Z",
     "start_time": "2023-09-04T09:19:46.343441823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>1707565129.7864115</td>\n",
       "      <td>DSCVEqoUN2KkdA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>1707565129.7864115</td>\n",
       "      <td>IpwvkBzTBPz4gQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name        _dlt_load_id         _dlt_id\n",
       "0   1  Alice  1707565129.7864115  DSCVEqoUN2KkdA\n",
       "1   2    Bob  1707565129.7864115  IpwvkBzTBPz4gQ"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "data_table = conn.sql(\"SELECT * FROM users\").df()\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b26d1-6b37-42dd-90ac-a1cd64106254",
   "metadata": {},
   "source": [
    "## Declare loading behavior\n",
    "\n",
    "You can finetune the loading process by decorating Python functions with `@dlt.resource`.\n",
    "\n",
    "### Load only new data (incremental loading)\n",
    "\n",
    "We can supercharge the example above and get only users that were created since last load.\n",
    "Instead of using `replace` write_disposition and downloading all users each time the pipeline is run, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27e3232f-c120-4a68-81e4-567f2ec823fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:22:02.472937821Z",
     "start_time": "2023-09-04T09:22:02.164647188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline incremental load step completed in 0.37 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/incremental.duckdb location to store data\n",
      "Load package 1707571366.9300675 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "data = [\n",
    "\t{'id': 1, 'name': 'Alice', 'created_at': \"2023-09-01\"},\n",
    "\t{'id': 2, 'name': 'Bob', 'created_at': \"2023-09-02\"},\n",
    "    {'id': 3, 'name': 'Chad', 'created_at': \"2023-09-03\"},\n",
    "    {'id': 4, 'name': 'Carol', 'created_at': \"2023-09-04\"}\n",
    "]\n",
    "\n",
    "@dlt.resource\n",
    "def users(\n",
    "    created_at=dlt.sources.incremental(\"created_at\", initial_value=\"2023-08-01\")\n",
    "):\n",
    "    yield from data\n",
    "    \n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='incremental',\n",
    "\tdestination='duckdb',\n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=False, \n",
    ")\n",
    "load_info = pipeline.run(users)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262c3ea-e867-4928-9ced-6c83b2a83b72",
   "metadata": {},
   "source": [
    "We use the `@dlt.resource` decorator to declare table name to which data will be loaded and write disposition, which is `append` by default.\n",
    "\n",
    "We also use `dlt.sources.incremental` to track `created_at` field present in each user to filter only the newly created ones.\n",
    "\n",
    "Now run the script. It loads all the users from our test data to `duckdb`. Run it again, and you can see that no users got added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a46cb2a-0343-4055-bbd4-a1a820633b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:22:04.129407948Z",
     "start_time": "2023-09-04T09:22:04.090087062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>1707571366.9300675</td>\n",
       "      <td>EfYj9z8D3FECFw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>2023-09-02</td>\n",
       "      <td>1707571366.9300675</td>\n",
       "      <td>pHAxCj2yDI4NWQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Chad</td>\n",
       "      <td>2023-09-03</td>\n",
       "      <td>1707571366.9300675</td>\n",
       "      <td>jklX34Tm3oWh6g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Carol</td>\n",
       "      <td>2023-09-04</td>\n",
       "      <td>1707571366.9300675</td>\n",
       "      <td>U+FXAr5VjHYpjw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name  created_at        _dlt_load_id         _dlt_id\n",
       "0   1  Alice  2023-09-01  1707571366.9300675  EfYj9z8D3FECFw\n",
       "1   2    Bob  2023-09-02  1707571366.9300675  pHAxCj2yDI4NWQ\n",
       "2   3   Chad  2023-09-03  1707571366.9300675  jklX34Tm3oWh6g\n",
       "3   4  Carol  2023-09-04  1707571366.9300675  U+FXAr5VjHYpjw"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "data_table = conn.sql(\"SELECT * FROM users\").df()\n",
    "data_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729bbf5-237e-4de4-b765-16af9bbac335",
   "metadata": {},
   "source": [
    "## Update and deduplicate your data\n",
    "\n",
    "The script above finds new users and adds them to the database.\n",
    "It will ignore any updates to user information.\n",
    "Get always fresh content of all the users: combine an incremental load with `merge` write disposition,\n",
    "like in the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d60eafe894de988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:26:01.833981447Z",
     "start_time": "2023-09-04T09:26:01.745826974Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline merge load step completed in 0.67 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/merge.duckdb location to store data\n",
      "Load package 1707571404.8401496 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "data = [\n",
    "\t{'id': 1, 'name': 'Alice', 'created_at': \"2023-09-01\", 'updated_at': \"2023-09-01\"},\n",
    "\t{'id': 2, 'name': 'Boba', 'created_at': \"2023-09-02\", 'updated_at': \"2023-09-05\"},\n",
    "    {'id': 3, 'name': 'Chad', 'created_at': \"2023-09-03\", 'updated_at': \"2023-09-03\"},\n",
    "    {'id': 4, 'name': 'Carol', 'created_at': \"2023-09-04\", 'updated_at': \"2023-09-04\"}\n",
    "]\n",
    "\n",
    "@dlt.resource(\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"id\",\n",
    ")\n",
    "def users(\n",
    "    updated_at=dlt.sources.incremental(\"updated_at\", initial_value=\"2023-08-01\")\n",
    "):\n",
    "    yield from data\n",
    "    \n",
    "pipeline = dlt.pipeline(\n",
    "\tpipeline_name='merge',\n",
    "\tdestination='duckdb',\n",
    "\tdataset_name='mydata',\n",
    "    full_refresh=False, \n",
    ")\n",
    "load_info = pipeline.run(users)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203c3dfb6e084cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Above we add `primary_key` hint that tells `dlt` how to identify the users in the database to find duplicates which content it will merge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4622c01a6fdc9196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:26:03.529032144Z",
     "start_time": "2023-09-04T09:26:03.452750779Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Carol</td>\n",
       "      <td>2023-09-04</td>\n",
       "      <td>2023-09-04</td>\n",
       "      <td>1707571404.8401496</td>\n",
       "      <td>HVFQgPOBENAxAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>1707571404.8401496</td>\n",
       "      <td>MvxcK4MGB6BziQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Boba</td>\n",
       "      <td>2023-09-02</td>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>1707571404.8401496</td>\n",
       "      <td>eiwUPKwI2eRmLQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Chad</td>\n",
       "      <td>2023-09-03</td>\n",
       "      <td>2023-09-03</td>\n",
       "      <td>1707571404.8401496</td>\n",
       "      <td>TQiBo0jrGu1Enw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name  created_at  updated_at        _dlt_load_id         _dlt_id\n",
       "0   4  Carol  2023-09-04  2023-09-04  1707571404.8401496  HVFQgPOBENAxAQ\n",
       "1   1  Alice  2023-09-01  2023-09-01  1707571404.8401496  MvxcK4MGB6BziQ\n",
       "2   2   Boba  2023-09-02  2023-09-05  1707571404.8401496  eiwUPKwI2eRmLQ\n",
       "3   3   Chad  2023-09-03  2023-09-03  1707571404.8401496  TQiBo0jrGu1Enw"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "data_table = conn.sql(\"SELECT * FROM users\").df()\n",
    "data_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af94689a376313",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Real life example\n",
    "\n",
    "We can improve the GitHub API example above and get only issues that were created since last load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69ab3088-9cf1-4644-af47-e082e38f6756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:28:05.593605528Z",
     "start_time": "2023-09-04T09:28:02.249099614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline github_issues_merge load step completed in 1.17 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/github_issues_merge.duckdb location to store data\n",
      "Load package 1707571416.876398 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import requests\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"issues\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"id\",\n",
    ")\n",
    "def get_issues(\n",
    "    updated_at = dlt.sources.incremental(\"updated_at\", initial_value=\"1970-01-01T00:00:00Z\")\n",
    "):\n",
    "    # url to request dlt-hub issues\n",
    "    url = f\"https://api.github.com/repos/dlt-hub/dlt/issues?since={updated_at.last_value}\"\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        page_items = response.json()\n",
    "\n",
    "        if len(page_items) == 0:\n",
    "            break\n",
    "        yield page_items\n",
    "\n",
    "        if \"next\" not in response.links:\n",
    "            break\n",
    "        url = response.links[\"next\"][\"url\"]\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='github_issues_merge',\n",
    "    destination='duckdb',\n",
    "    dataset_name='mydata',\n",
    "    full_refresh=False,\n",
    ")\n",
    "# dlt works with lists of dicts, so wrap data to the list\n",
    "load_info = pipeline.run(get_issues)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32471e-31d9-4571-9c89-cbf76df9bfba",
   "metadata": {},
   "source": [
    "\n",
    "Note that we now track the `updated_at` field - so we filter in all issues **updated** since the last pipeline run (which also includes newly created ones).\n",
    "\n",
    "Also pay attention how we use **since** [GitHub API](https://docs.github.com/en/rest/issues/issues?apiVersion=2022-11-28#list-repository-issues)\n",
    "and `updated_at.last_value` to tell GitHub which issues we are interested in. `updated_at.last_value` holds the last `updated_at` value from the previous run.\n",
    "\n",
    "Now you can run this script on a daily schedule, and each day you'll load only issues created after the time of the previous pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f0d5253-d549-43a9-9104-cca803770bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:28:09.446527782Z",
     "start_time": "2023-09-04T09:28:09.371351130Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67847/2972541876.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data_table = conn.sql(\"SELECT * FROM issues\").df()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>repository_url</th>\n",
       "      <th>labels_url</th>\n",
       "      <th>comments_url</th>\n",
       "      <th>events_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>node_id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>performed_via_github_app__permissions__actions</th>\n",
       "      <th>performed_via_github_app__permissions__checks</th>\n",
       "      <th>performed_via_github_app__permissions__contents</th>\n",
       "      <th>performed_via_github_app__permissions__deployments</th>\n",
       "      <th>performed_via_github_app__permissions__discussions</th>\n",
       "      <th>performed_via_github_app__permissions__issues</th>\n",
       "      <th>performed_via_github_app__permissions__metadata</th>\n",
       "      <th>performed_via_github_app__permissions__pull_requests</th>\n",
       "      <th>performed_via_github_app__permissions__repository_projects</th>\n",
       "      <th>performed_via_github_app__permissions__statuses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2126291188</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://github.com/dlt-hub/dlt/pull/954</td>\n",
       "      <td>PR_kwDOGvRYu85mb7da</td>\n",
       "      <td>954</td>\n",
       "      <td>Add git to filesystem source 301</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2126159467</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://github.com/dlt-hub/dlt/pull/953</td>\n",
       "      <td>PR_kwDOGvRYu85mbfKI</td>\n",
       "      <td>953</td>\n",
       "      <td>passes incremental from apply hints to resourc...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2126134270</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://github.com/dlt-hub/dlt/pull/952</td>\n",
       "      <td>PR_kwDOGvRYu85mbZvc</td>\n",
       "      <td>952</td>\n",
       "      <td>855 create bigquery adapter for dlt resources</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2125791707</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://github.com/dlt-hub/dlt/pull/951</td>\n",
       "      <td>PR_kwDOGvRYu85maOZo</td>\n",
       "      <td>951</td>\n",
       "      <td>Handle UnionType when checking is_union_type a...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2125375604</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://api.github.com/repos/dlt-hub/dlt/issue...</td>\n",
       "      <td>https://github.com/dlt-hub/dlt/pull/950</td>\n",
       "      <td>PR_kwDOGvRYu85mYxul</td>\n",
       "      <td>950</td>\n",
       "      <td>Tweaked output to not include \"Found schema\" a...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                                url  \\\n",
       "0  2126291188  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "1  2126159467  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "2  2126134270  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "3  2125791707  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "4  2125375604  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "\n",
       "                             repository_url  \\\n",
       "0  https://api.github.com/repos/dlt-hub/dlt   \n",
       "1  https://api.github.com/repos/dlt-hub/dlt   \n",
       "2  https://api.github.com/repos/dlt-hub/dlt   \n",
       "3  https://api.github.com/repos/dlt-hub/dlt   \n",
       "4  https://api.github.com/repos/dlt-hub/dlt   \n",
       "\n",
       "                                          labels_url  \\\n",
       "0  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "1  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "2  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "3  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "4  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "\n",
       "                                        comments_url  \\\n",
       "0  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "1  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "2  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "3  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "4  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "\n",
       "                                          events_url  \\\n",
       "0  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "1  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "2  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "3  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "4  https://api.github.com/repos/dlt-hub/dlt/issue...   \n",
       "\n",
       "                                  html_url              node_id  number  \\\n",
       "0  https://github.com/dlt-hub/dlt/pull/954  PR_kwDOGvRYu85mb7da     954   \n",
       "1  https://github.com/dlt-hub/dlt/pull/953  PR_kwDOGvRYu85mbfKI     953   \n",
       "2  https://github.com/dlt-hub/dlt/pull/952  PR_kwDOGvRYu85mbZvc     952   \n",
       "3  https://github.com/dlt-hub/dlt/pull/951  PR_kwDOGvRYu85maOZo     951   \n",
       "4  https://github.com/dlt-hub/dlt/pull/950  PR_kwDOGvRYu85mYxul     950   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0                   Add git to filesystem source 301  ...   \n",
       "1  passes incremental from apply hints to resourc...  ...   \n",
       "2      855 create bigquery adapter for dlt resources  ...   \n",
       "3  Handle UnionType when checking is_union_type a...  ...   \n",
       "4  Tweaked output to not include \"Found schema\" a...  ...   \n",
       "\n",
       "  performed_via_github_app__permissions__actions  \\\n",
       "0                                           None   \n",
       "1                                           None   \n",
       "2                                           None   \n",
       "3                                           None   \n",
       "4                                           None   \n",
       "\n",
       "   performed_via_github_app__permissions__checks  \\\n",
       "0                                           None   \n",
       "1                                           None   \n",
       "2                                           None   \n",
       "3                                           None   \n",
       "4                                           None   \n",
       "\n",
       "  performed_via_github_app__permissions__contents  \\\n",
       "0                                            None   \n",
       "1                                            None   \n",
       "2                                            None   \n",
       "3                                            None   \n",
       "4                                            None   \n",
       "\n",
       "  performed_via_github_app__permissions__deployments  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "  performed_via_github_app__permissions__discussions  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "  performed_via_github_app__permissions__issues  \\\n",
       "0                                          None   \n",
       "1                                          None   \n",
       "2                                          None   \n",
       "3                                          None   \n",
       "4                                          None   \n",
       "\n",
       "  performed_via_github_app__permissions__metadata  \\\n",
       "0                                            None   \n",
       "1                                            None   \n",
       "2                                            None   \n",
       "3                                            None   \n",
       "4                                            None   \n",
       "\n",
       "  performed_via_github_app__permissions__pull_requests  \\\n",
       "0                                               None     \n",
       "1                                               None     \n",
       "2                                               None     \n",
       "3                                               None     \n",
       "4                                               None     \n",
       "\n",
       "  performed_via_github_app__permissions__repository_projects  \\\n",
       "0                                               None           \n",
       "1                                               None           \n",
       "2                                               None           \n",
       "3                                               None           \n",
       "4                                               None           \n",
       "\n",
       "  performed_via_github_app__permissions__statuses  \n",
       "0                                            None  \n",
       "1                                            None  \n",
       "2                                            None  \n",
       "3                                            None  \n",
       "4                                            None  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "data_table = conn.sql(\"SELECT * FROM issues\").df()\n",
    "data_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561db25-5296-4d5b-8466-1b860f8ef867",
   "metadata": {},
   "source": [
    "### Use existed verified sources\n",
    "\n",
    "To use existed verified source, just run the `dlt init` [command](https://dlthub.com/docs/reference/command-line-interface#dlt-init).\n",
    "\n",
    "List all verified sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a79fa0d-ade6-4073-beeb-779ddf13e34c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:29:10.334136234Z",
     "start_time": "2023-09-04T09:29:06.772469428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up for verified sources in \u001b[1mhttps://github.com/dlt-hub/verified-sources.git\u001b[0m...\n",
      "\u001b[1mpipedrive\u001b[0m: Highly customizable source for Pipedrive, supports endpoint addition, selection and column rename\n",
      "\u001b[1mnotion\u001b[0m: A source that extracts data from Notion API\n",
      "\u001b[1msalesforce\u001b[0m: Source for Salesforce depending on the simple_salesforce python package.\n",
      "\u001b[1mhubspot\u001b[0m: This is a module that provides a DLT source to retrieve data from multiple endpoints of the HubSpot API using a specified API key. The retrieved data is returned as a tuple of Dlt resources, one for each endpoint.\n",
      "\u001b[1mgithub\u001b[0m: Source that load github issues, pull requests and reactions for a specific repository via customizable graphql query. Loads events incrementally.\n",
      "\u001b[1mchess\u001b[0m: A source loading player profiles and games from chess.com api\n",
      "\u001b[1mstripe_analytics\u001b[0m: This source uses Stripe API and dlt to load data such as Customer, Subscription, Event etc. to the database and to calculate the MRR and churn rate. \n",
      "\u001b[1mshopify_dlt\u001b[0m: Fetches Shopify Orders and Products.\n",
      "\u001b[1mzendesk\u001b[0m: Defines all the sources and resources needed for ZendeskSupport, ZendeskChat and ZendeskTalk\n",
      "\u001b[1mworkable\u001b[0m: This source uses Workable API and dlt to load data such as Candidates, Jobs, Events, etc. to the database.\n",
      "\u001b[1mmatomo\u001b[0m: Loads reports and raw visits data from Matomo\n",
      "\u001b[1mfilesystem\u001b[0m: Reads files in s3, gs or azure buckets using fsspec and provides convenience resources for chunked reading of various file formats\n",
      "\u001b[1mpokemon\u001b[0m: This source provides data extraction from an example source as a starting point for new pipelines.\n",
      "\u001b[1mpersonio\u001b[0m: Fetches Personio Employees, Absences, Attendances.\n",
      "\u001b[1mmongodb\u001b[0m: Source that loads collections form any a mongo database, supports incremental loads.\n",
      "\u001b[1mgoogle_analytics\u001b[0m: Defines all the sources and resources needed for Google Analytics V4\n",
      "\u001b[1mjira\u001b[0m: This source uses Jira API and dlt to load data such as Issues, Users, Workflows and Projects to the database. \n",
      "\u001b[1mgoogle_sheets\u001b[0m: Loads Google Sheets data from tabs, named and explicit ranges. Contains the main source functions.\n",
      "\u001b[1munstructured_data\u001b[0m: This source converts unstructured data from a specified data resource to structured data using provided queries.\n",
      "\u001b[1minbox\u001b[0m: Reads messages and attachments from e-mail inbox via IMAP protocol\n",
      "\u001b[1mstrapi\u001b[0m: Basic strapi source\n",
      "\u001b[1mkafka\u001b[0m: A source to extract Kafka messages.\u001b[33m [needs update: dlt<0.4,>=0.3.25]\u001b[0m\n",
      "\u001b[1mslack\u001b[0m: Fetches Slack Conversations, History and logs.\n",
      "\u001b[1masana_dlt\u001b[0m: This source provides data extraction from the Asana platform via their API.\n",
      "\u001b[1mkinesis\u001b[0m: Reads messages from Kinesis queue.\n",
      "\u001b[1mmux\u001b[0m: Loads Mux views data using https://docs.mux.com/api-reference\n",
      "\u001b[1mfacebook_ads\u001b[0m: Loads campaigns, ads sets, ads, leads and insight data from Facebook Marketing API\n",
      "\u001b[1msql_database\u001b[0m: Source that loads tables form any SQLAlchemy supported database, supports batching requests and incremental loads.\n",
      "\u001b[1mairtable\u001b[0m: Source that loads tables form Airtable.\n"
     ]
    }
   ],
   "source": [
    "!dlt init --list-verified-sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d7616-f4ed-46a6-b53d-efe7d01726d8",
   "metadata": {},
   "source": [
    "This command shows all available verified sources and their short descriptions. For each source, checks if your local `dlt` version requires update and prints the relevant warning.\n",
    "\n",
    "Consider an example of a pipeline for Pokemon API.\n",
    "\n",
    "This command will initialize the pipeline example with Pokemon as the source and `duckdb` as the [destination](https://dlthub.com/docs/dlt-ecosystem/destinations):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecabaf6a-307a-4c9c-ac65-75c177536672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:29:29.443647544Z",
     "start_time": "2023-09-04T09:29:26.409082099Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up the init scripts in \u001b[1mhttps://github.com/dlt-hub/verified-sources.git\u001b[0m...\n",
      "Cloning and configuring a verified source \u001b[1mpokemon\u001b[0m (This source provides data extraction from an example source as a starting point for new pipelines.)\n",
      "\n",
      "Verified source \u001b[1mpokemon\u001b[0m was added to your project!\n",
      "* See the usage examples and code snippets to copy from \u001b[1mpokemon_pipeline.py\u001b[0m\n",
      "* Add credentials for \u001b[1mduckdb\u001b[0m and other secrets in \u001b[1m./.dlt/secrets.toml\u001b[0m\n",
      "* \u001b[1mrequirements.txt\u001b[0m was created. Install it with:\n",
      "pip3 install -r requirements.txt\n",
      "* Read \u001b[1mhttps://dlthub.com/docs/walkthroughs/create-a-pipeline\u001b[0m for more information\n"
     ]
    }
   ],
   "source": [
    "!dlt --non-interactive init pokemon duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ea9297-c1e4-4de0-a301-0639fa86358f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:30:42.803517788Z",
     "start_time": "2023-09-04T09:30:38.208807809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline pokemon load step completed in 0.40 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset pokemon_data\n",
      "The duckdb destination used duckdb:////mnt/c/Users/ellabelle/Github/data-engineering-zoomcamp/cohorts/2024/workshops/dlt_resources/pokemon.duckdb location to store data\n",
      "Load package 1707571327.2131693 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "!python pokemon_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ced5c7-c2b2-47de-bcf6-ca8e0900d9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T09:30:43.071298211Z",
     "start_time": "2023-09-04T09:30:43.004537860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬──────────────┬─────────────────────┬──────────────────────┬───────────────────────────────────┬───────────┐\n",
       "│ database │    schema    │        name         │     column_names     │           column_types            │ temporary │\n",
       "│ varchar  │   varchar    │       varchar       │      varchar[]       │             varchar[]             │  boolean  │\n",
       "├──────────┼──────────────┼─────────────────────┼──────────────────────┼───────────────────────────────────┼───────────┤\n",
       "│ pokemon  │ pokemon_data │ _dlt_loads          │ [load_id, schema_n…  │ [VARCHAR, VARCHAR, BIGINT, TIME…  │ false     │\n",
       "│ pokemon  │ pokemon_data │ _dlt_pipeline_state │ [version, engine_v…  │ [BIGINT, BIGINT, VARCHAR, VARCH…  │ false     │\n",
       "│ pokemon  │ pokemon_data │ _dlt_version        │ [version, engine_v…  │ [BIGINT, BIGINT, TIMESTAMP WITH…  │ false     │\n",
       "│ pokemon  │ pokemon_data │ berries             │ [name, url, _dlt_l…  │ [VARCHAR, VARCHAR, VARCHAR, VAR…  │ false     │\n",
       "│ pokemon  │ pokemon_data │ pokemon             │ [name, url, _dlt_l…  │ [VARCHAR, VARCHAR, VARCHAR, VAR…  │ false     │\n",
       "└──────────┴──────────────┴─────────────────────┴──────────────────────┴───────────────────────────────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bulbasaur</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/1/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>kRcAPvqYNKoAxg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ivysaur</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/2/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>PzSkfn5ScYOu4g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>venusaur</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/3/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>AN5lJasTDa5HTw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>charmander</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/4/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>C36D/mh95FyE2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>charmeleon</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/5/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>pcSsexwQBy6qrw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>charizard</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/6/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>Sy3UEvw0OUVWqg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>squirtle</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/7/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>cIRaNmP5o+6log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wartortle</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/8/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>WriOrmND60uVow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blastoise</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/9/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>4PiuBk0tYuHCEw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>caterpie</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/10/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>7ul6Wt4IjW/uaA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>metapod</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/11/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>wLeOYjROdBcPCw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>butterfree</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/12/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>y0YusUrtgQRxlA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>weedle</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/13/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>/wUWCLPkaCXB7g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kakuna</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/14/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>dZMQ1ohLcLN0Gw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>beedrill</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/15/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>Y/UsBxChf2rdrg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pidgey</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/16/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>SX8xrVRlmo/0Lw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pidgeotto</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/17/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>klsdbY8FWp7lwQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pidgeot</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/18/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>Up2u2OJ6NmEpCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rattata</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/19/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>nqqRzl1kaxSA+g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raticate</td>\n",
       "      <td>https://pokeapi.co/api/v2/pokemon/20/</td>\n",
       "      <td>1707571327.2131693</td>\n",
       "      <td>xkE78A6pOwuHkA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name                                    url        _dlt_load_id  \\\n",
       "0    bulbasaur   https://pokeapi.co/api/v2/pokemon/1/  1707571327.2131693   \n",
       "1      ivysaur   https://pokeapi.co/api/v2/pokemon/2/  1707571327.2131693   \n",
       "2     venusaur   https://pokeapi.co/api/v2/pokemon/3/  1707571327.2131693   \n",
       "3   charmander   https://pokeapi.co/api/v2/pokemon/4/  1707571327.2131693   \n",
       "4   charmeleon   https://pokeapi.co/api/v2/pokemon/5/  1707571327.2131693   \n",
       "5    charizard   https://pokeapi.co/api/v2/pokemon/6/  1707571327.2131693   \n",
       "6     squirtle   https://pokeapi.co/api/v2/pokemon/7/  1707571327.2131693   \n",
       "7    wartortle   https://pokeapi.co/api/v2/pokemon/8/  1707571327.2131693   \n",
       "8    blastoise   https://pokeapi.co/api/v2/pokemon/9/  1707571327.2131693   \n",
       "9     caterpie  https://pokeapi.co/api/v2/pokemon/10/  1707571327.2131693   \n",
       "10     metapod  https://pokeapi.co/api/v2/pokemon/11/  1707571327.2131693   \n",
       "11  butterfree  https://pokeapi.co/api/v2/pokemon/12/  1707571327.2131693   \n",
       "12      weedle  https://pokeapi.co/api/v2/pokemon/13/  1707571327.2131693   \n",
       "13      kakuna  https://pokeapi.co/api/v2/pokemon/14/  1707571327.2131693   \n",
       "14    beedrill  https://pokeapi.co/api/v2/pokemon/15/  1707571327.2131693   \n",
       "15      pidgey  https://pokeapi.co/api/v2/pokemon/16/  1707571327.2131693   \n",
       "16   pidgeotto  https://pokeapi.co/api/v2/pokemon/17/  1707571327.2131693   \n",
       "17     pidgeot  https://pokeapi.co/api/v2/pokemon/18/  1707571327.2131693   \n",
       "18     rattata  https://pokeapi.co/api/v2/pokemon/19/  1707571327.2131693   \n",
       "19    raticate  https://pokeapi.co/api/v2/pokemon/20/  1707571327.2131693   \n",
       "\n",
       "           _dlt_id  \n",
       "0   kRcAPvqYNKoAxg  \n",
       "1   PzSkfn5ScYOu4g  \n",
       "2   AN5lJasTDa5HTw  \n",
       "3   C36D/mh95FyE2g  \n",
       "4   pcSsexwQBy6qrw  \n",
       "5   Sy3UEvw0OUVWqg  \n",
       "6   cIRaNmP5o+6log  \n",
       "7   WriOrmND60uVow  \n",
       "8   4PiuBk0tYuHCEw  \n",
       "9   7ul6Wt4IjW/uaA  \n",
       "10  wLeOYjROdBcPCw  \n",
       "11  y0YusUrtgQRxlA  \n",
       "12  /wUWCLPkaCXB7g  \n",
       "13  dZMQ1ohLcLN0Gw  \n",
       "14  Y/UsBxChf2rdrg  \n",
       "15  SX8xrVRlmo/0Lw  \n",
       "16  klsdbY8FWp7lwQ  \n",
       "17  Up2u2OJ6NmEpCA  \n",
       "18  nqqRzl1kaxSA+g  \n",
       "19  xkE78A6pOwuHkA  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"pokemon.duckdb\")\n",
    "conn.sql(f\"SET search_path = 'pokemon_data'\")\n",
    "display(conn.sql(\"DESCRIBE\"))\n",
    "data_table = conn.sql(\"SELECT * FROM pokemon\").df()\n",
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc87f805b82a941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T17:58:39.038837170Z",
     "start_time": "2023-09-02T17:58:39.036761770Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
