{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzi0xTYb4Duq"
      },
      "source": [
        "# Part 2: Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwRYRLce9i6"
      },
      "source": [
        "## Load nested data with auto normalisation\n",
        "\n",
        "When converting nested data to tabular formats, to keep fragmentations minimal:\n",
        "* Nested dictionaries can be flattened into the parent row to\n",
        "* Nested lists however need to be expressed as separate tables due to the different granularity (1:n relationship)\n",
        "\n",
        "And of course, when going from JSON to DB, we want some things standardised:\n",
        "* Data types such as timestamps should be detected correctly\n",
        "* Column names should be converted to db-compatible names\n",
        "* Unnested sub-tables should be linked to parent tables via auto generated keys\n",
        "\n",
        "\n",
        "For this work, we will use `dlt` library, which is purpose-made to solve such tasks in a scalable way, for example by using generators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoTtwE5het4C"
      },
      "source": [
        "### Introducing dlt\n",
        "\n",
        "dlt is a python library created for the purpose of assisting data engineers to build simpler, faster and more robust pipelines with minimal effort.\n",
        "\n",
        "dlt automates much of the tedious work a data engineer would do, and does it in a way that is robust.\n",
        "\n",
        "dlt can handle things like:\n",
        "\n",
        "- Schema: Inferring and evolving schema, alerting changes, using schemas as data contracts.\n",
        "- Typing data, flattening structures, renaming columns to fit database standards.\n",
        "- Processing a stream of events/rows without filling memory. This includes extraction from generators. In our example we will pass the “data” you can see above.\n",
        "- Loading to a variety of dbs of file formats.\n",
        "\n",
        "Read more about dlt [here](https://dlthub.com/docs/intro).\n",
        "\n",
        "Now let’s use it to load our nested json to duckdb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i07XAdb67H5",
        "outputId": "efb9979a-e3f9-469d-9bae-842533226c53"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import duckdb\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        \"vendor_name\": \"VTS\",\n",
        "\t\t\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
        "        \"time\": {\n",
        "            \"pickup\": \"2009-06-14 23:23:00\",\n",
        "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
        "        },\n",
        "        \"Trip_Distance\": 17.52,\n",
        "        # nested dictionaries could be flattened\n",
        "        \"coordinates\": { # coordinates__start__lon\n",
        "            \"start\": {\n",
        "                \"lon\": -73.787442,\n",
        "                \"lat\": 40.641525\n",
        "            },\n",
        "            \"end\": {\n",
        "                \"lon\": -73.980072,\n",
        "                \"lat\": 40.742963\n",
        "            }\n",
        "        },\n",
        "        \"Rate_Code\": None,\n",
        "        \"store_and_forward\": None,\n",
        "        \"Payment\": {\n",
        "            \"type\": \"Credit\",\n",
        "            \"amt\": 20.5,\n",
        "            \"surcharge\": 0,\n",
        "            \"mta_tax\": None,\n",
        "            \"tip\": 9,\n",
        "            \"tolls\": 4.15,\n",
        "\t\t\t\t\t\t\"status\": \"booked\"\n",
        "        },\n",
        "        \"Passenger_Count\": 2,\n",
        "        # nested lists need to be expressed as separate tables\n",
        "        \"passengers\": [\n",
        "            {\"name\": \"John\", \"rating\": 4.9},\n",
        "            {\"name\": \"Jack\", \"rating\": 3.9}\n",
        "        ],\n",
        "        \"Stops\": [\n",
        "            {\"lon\": -73.6, \"lat\": 40.6},\n",
        "            {\"lon\": -73.5, \"lat\": 40.5}\n",
        "        ]\n",
        "    },\n",
        "    # ... more data\n",
        "]\n",
        "\n",
        "\n",
        "# define the connection to load to.\n",
        "# We now use duckdb, but you can switch to Bigquery later\n",
        "pipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n",
        "\n",
        "\n",
        "\n",
        "# run with merge write disposition.\n",
        "# This is so scaffolding is created for the next example,\n",
        "# where we look at merging data\n",
        "\n",
        "info = pipeline.run(data,\n",
        "\t\t\t\t\ttable_name=\"rides\",\n",
        "\t\t\t\t\twrite_disposition=\"merge\",\n",
        "                    primary_key=\"record_hash\")\n",
        "\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbORAD3Rhsr1"
      },
      "source": [
        "### Inspecting the nested structure, joining the child tables\n",
        "\n",
        "Let's look at what happened during the load\n",
        "- By looking at the loaded tables, we can see our json document got flattened and sub-documents got split into separate tables\n",
        "- We can re-join those child tables to the parent table by using the generated keys `on parent_table._dlt_id = child_table._dlt_parent_id`.\n",
        "- Data types: If you will pay attention to datatypes, you will note that the timestamps, which in json are of string type, are now of timestamp type in the db.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Irf4XsfcgqQv",
        "outputId": "9a746b0b-7a92-463b-85ae-63ee3458a25e"
      },
      "outputs": [],
      "source": [
        "# show the outcome\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\n",
        "rides = conn.sql(\"SELECT * FROM rides\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n Pasengers table\")\n",
        "passengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\n",
        "display(passengers)\n",
        "print(\"\\n\\n\\n Stops table\")\n",
        "stops = conn.sql(\"SELECT * FROM rides__stops\").df()\n",
        "display(stops)\n",
        "\n",
        "\n",
        "# to reflect the relationships between parent and child rows, let's join them\n",
        "# of course this will have 4 rows due to the two 1:n joins\n",
        "\n",
        "print(\"\\n\\n\\n joined table\")\n",
        "\n",
        "joined = conn.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM rides as r\n",
        "left join rides__passengers as rp\n",
        "  on r._dlt_id = rp._dlt_parent_id\n",
        "left join rides__stops as rs\n",
        "  on r._dlt_id = rs._dlt_parent_id\n",
        "\"\"\").df()\n",
        "display(joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duR1wFIfbM07"
      },
      "source": [
        "What are we looking at?\n",
        "- Nested dicts got flattened into the parent row, the structure `{\"coordinates\":{\"start\": {\"lat\": ...}}}` became\n",
        "`coordinates__start__lat`\n",
        "\n",
        "- Nested lists got broken out into separate tables with generated columns that would allow us to join the data back when needed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
