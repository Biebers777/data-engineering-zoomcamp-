{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Irf4XsfcgqQv",
        "outputId": "9a746b0b-7a92-463b-85ae-63ee3458a25e"
      },
      "outputs": [],
      "source": [
        "# show the outcome\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\n",
        "rides = conn.sql(\"SELECT * FROM rides\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n Pasengers table\")\n",
        "passengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\n",
        "display(passengers)\n",
        "print(\"\\n\\n\\n Stops table\")\n",
        "stops = conn.sql(\"SELECT * FROM rides__stops\").df()\n",
        "display(stops)\n",
        "\n",
        "\n",
        "# to reflect the relationships between parent and child rows, let's join them\n",
        "# of course this will have 4 rows due to the two 1:n joins\n",
        "\n",
        "print(\"\\n\\n\\n joined table\")\n",
        "\n",
        "joined = conn.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM rides as r\n",
        "left join rides__passengers as rp\n",
        "  on r._dlt_id = rp._dlt_parent_id\n",
        "left join rides__stops as rs\n",
        "  on r._dlt_id = rs._dlt_parent_id\n",
        "\"\"\").df()\n",
        "display(joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duR1wFIfbM07"
      },
      "source": [
        "What are we looking at?\n",
        "- Nested dicts got flattened into the parent row, the structure `{\"coordinates\":{\"start\": {\"lat\": ...}}}` became\n",
        "`coordinates__start__lat`\n",
        "\n",
        "- Nested lists got broken out into separate tables with generated columns that would allow us to join the data back when needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDpgADmSBdr9"
      },
      "source": [
        "# Part 3: Incremental loading\n",
        "## Update nested data\n",
        "\n",
        "In this example the scores of the 2 passengers changed. Turns out their payment didn't go through for the ride before and they got a bad rating from the driver, so now we have to update their rating.\n",
        "\n",
        "As you can see after running the code, their ratings are now lowered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "D3W5iXow8cs8",
        "outputId": "1191508d-09cd-43ed-a68a-c35e49d9edc6"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import duckdb\n",
        "\n",
        "data = [\n",
        "    {\n",
        "        \"vendor_name\": \"VTS\",\n",
        "\t\t\t\t\"record_hash\": \"b00361a396177a9cb410ff61f20015ad\",\n",
        "        \"time\": {\n",
        "            \"pickup\": \"2009-06-14 23:23:00\",\n",
        "            \"dropoff\": \"2009-06-14 23:48:00\"\n",
        "        },\n",
        "        \"Trip_Distance\": 17.52,\n",
        "        \"coordinates\": {\n",
        "            \"start\": {\n",
        "                \"lon\": -73.787442,\n",
        "                \"lat\": 40.641525\n",
        "            },\n",
        "            \"end\": {\n",
        "                \"lon\": -73.980072,\n",
        "                \"lat\": 40.742963\n",
        "            }\n",
        "        },\n",
        "        \"Rate_Code\": None,\n",
        "        \"store_and_forward\": None,\n",
        "        \"Payment\": {\n",
        "            \"type\": \"Credit\",\n",
        "            \"amt\": 20.5,\n",
        "            \"surcharge\": 0,\n",
        "            \"mta_tax\": None,\n",
        "            \"tip\": 9,\n",
        "            \"tolls\": 4.15,\n",
        "\t\t\t\t\t\t\"status\": \"cancelled\"\n",
        "        },\n",
        "        \"Passenger_Count\": 2,\n",
        "        \"passengers\": [\n",
        "            {\"name\": \"John\", \"rating\": 4.4},\n",
        "            {\"name\": \"Jack\", \"rating\": 3.6}\n",
        "        ],\n",
        "        \"Stops\": [\n",
        "            {\"lon\": -73.6, \"lat\": 40.6},\n",
        "            {\"lon\": -73.5, \"lat\": 40.5}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# define the connection to load to.\n",
        "# We now use duckdb, but you can switch to Bigquery later\n",
        "pipeline = dlt.pipeline(destination='duckdb', dataset_name='taxi_rides')\n",
        "\n",
        "# run the pipeline with default settings, and capture the outcome\n",
        "info = pipeline.run(data,\n",
        "\t\t\t\t\ttable_name=\"rides\",\n",
        "\t\t\t\t\twrite_disposition=\"merge\",\n",
        "                    primary_key='record_hash')\n",
        "\n",
        "# show the outcome\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# let's see the tables\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "print('Loaded tables: ')\n",
        "display(conn.sql(\"show tables\"))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n Rides table below: Note the times are properly typed\")\n",
        "rides = conn.sql(\"SELECT * FROM rides\").df()\n",
        "display(rides)\n",
        "\n",
        "print(\"\\n\\n\\n Pasengers table\")\n",
        "passengers = conn.sql(\"SELECT * FROM rides__passengers\").df()\n",
        "display(passengers)\n",
        "print(\"\\n\\n\\n Stops table\")\n",
        "stops = conn.sql(\"SELECT * FROM rides__stops\").df()\n",
        "display(stops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXw1A9QUDUJ"
      },
      "source": [
        "# Bonus snippets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp1tfRt1S4Gz"
      },
      "source": [
        "## Load to parquet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sj2aTU-GUe2J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘.dlt’: File exists\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "# !pip install dlt[parquet] # Install dlt with all the necessary DuckDB dependencies\n",
        "# !pip install parquet\n",
        "!mkdir .dlt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_and_read_jsonl(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    data = response.text.splitlines()\n",
        "    parsed_data = [json.loads(line) for line in data]\n",
        "    return parsed_data\n",
        "\n",
        "def stream_download_jsonl(url):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    for line in response.iter_lines():\n",
        "        if line:\n",
        "            yield json.loads(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ74NAUoV06Y",
        "outputId": "98c71750-8b0c-441e-c519-df22a4c7fdac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dlt\n",
        "import parquet\n",
        "import json\n",
        "import glob\n",
        "import requests\n",
        "import duckdb\n",
        "\n",
        "# Set the bucket_url. We can also use a local folder\n",
        "# os.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = 'file://.dlt/my_folder'\n",
        "\n",
        "url = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n",
        "# Define your pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name='my_pipeline',\n",
        "    destination='duckdb',\n",
        "    dataset_name='mydata',\n",
        "    full_refresh=False, \n",
        "    credentials=\"/.dlt/data.db\"\n",
        ")\n",
        "\n",
        "\n",
        "# Run the pipeline with the generator we created earlier.\n",
        "load_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\n",
        "\n",
        "print(load_info)\n",
        "\n",
        "# Get a list of all Parquet files in the specified folder\n",
        "# parquet_files = glob.glob('/.dlt/my_folder/*.parquet')\n",
        "\n",
        "# show parquet files\n",
        "# print(\"Loaded files: \")\n",
        "# for file in parquet_files:\n",
        "#   print(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoEk7pYXTBJe"
      },
      "source": [
        "## Load to bigquery\n",
        "To load to bigquery, we need credentials to bigquery.\n",
        "- dlt looks for credentials in several places as described in the [credential docs.](https://dlthub.com/docs/general-usage/credentials/configuration)\n",
        "- In the case of Bigquery you can read the docs [here](https://dlthub.com/docs/dlt-ecosystem/destinations/bigquery) for how to do it.\n",
        "- If you are running from Colab or a GCP machine, or you are authenticated with the gcp CLI, you can use these already-available local credentials. We will use the Colab Oauth here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIKdZERfxOYA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dlt[bigquery]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ3uWgqZxKS7"
      },
      "outputs": [],
      "source": [
        "# Authenticate to Google BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bjVf9QXTA8R",
        "outputId": "fadb2851-45f5-4b80-fda7-abc45343b2bd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dlt\n",
        "\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'dlt-dev-external-413811'\n",
        "\n",
        "\n",
        "# Define your pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name='my_pipeline',\n",
        "    destination='bigquery',\n",
        "    dataset_name='dtc'\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "load_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\")\n",
        "\n",
        "print(load_info)\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT *\n",
        "    FROM `dtc.users`\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)  # Make an API request.\n",
        "\n",
        "print(\"The query data:\")\n",
        "for row in query_job:\n",
        "    # Row values can be accessed by field name or index.\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bftPmKJYS_7J"
      },
      "source": [
        "## Other demos\n",
        "Find more demos in this repo, or look on our blog for multiple community demos\n",
        "* https://github.com/dlt-hub/dlt_demos\n",
        "* https://dlthub.com/docs/blog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPjk0O3S_Ag"
      },
      "source": [
        "## Docs Links\n",
        "\n",
        "This course was tailored to enable all the cohort to complete it succesfully - so more complex things were left out. We strongly encourage you to keep learning on your own.\n",
        "\n",
        "\n",
        "You will find more info about advanced capabilities of dlt here: https://dlthub.com/docs/build-a-pipeline-tutorial\n",
        "\n",
        "\n",
        "\n",
        "Don't miss the GPT-4 docs helper button - it will help with simple questions.\n",
        "\n",
        "If you get stuck, consider joining our community for help https://dlthub.com/community"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
