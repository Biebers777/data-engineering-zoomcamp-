{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae324d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66f42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/03/02 10:57:02 WARN Utils: Your hostname, Cinders resolves to a loopback address: 127.0.1.1; using 172.17.156.62 instead (on interface eth0)\n",
      "24/03/02 10:57:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/02 10:57:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "\t.master(\"local[*]\") \\\n",
    "\t.appName('test') \\\n",
    "\t.getOrCreate()\n",
    "\t\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646fc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('../../data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cccd5",
   "metadata": {},
   "source": [
    "```\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0498ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 14, 13, 28, 9), lpep_dropoff_datetime=datetime.datetime(2020, 1, 14, 13, 35, 19), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=75, passenger_count=1, trip_distance=1.35, fare_amount=7.0, extra=0.0, mta_tax=0.5, tip_amount=1.56, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=9.36, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 28, 12, 11, 44), lpep_dropoff_datetime=datetime.datetime(2020, 1, 28, 12, 25, 21), store_and_fwd_flag='N', RatecodeID=1, PULocationID=129, DOLocationID=179, passenger_count=1, trip_distance=3.73, fare_amount=13.5, extra=0.0, mta_tax=0.5, tip_amount=1.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=15.3, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 4, 11, 38), lpep_dropoff_datetime=datetime.datetime(2020, 1, 4, 12, 11), store_and_fwd_flag=None, RatecodeID=None, PULocationID=25, DOLocationID=41, passenger_count=None, trip_distance=11.19, fare_amount=28.38, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.0, total_amount=28.88, payment_type=None, trip_type=None, congestion_surcharge=None),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 30, 10, 3, 36), lpep_dropoff_datetime=datetime.datetime(2020, 1, 30, 10, 9, 55), store_and_fwd_flag='N', RatecodeID=5, PULocationID=182, DOLocationID=242, passenger_count=1, trip_distance=1.32, fare_amount=12.0, extra=0.0, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=12.3, payment_type=1, trip_type=2, congestion_surcharge=0.0),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 30, 20, 56, 17), lpep_dropoff_datetime=datetime.datetime(2020, 1, 30, 21, 0, 18), store_and_fwd_flag='N', RatecodeID=1, PULocationID=210, DOLocationID=210, passenger_count=1, trip_distance=0.8, fare_amount=5.0, extra=0.5, mta_tax=0.5, tip_amount=0.7, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=7.0, payment_type=1, trip_type=1, congestion_surcharge=0.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fe52cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324d871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 14, 13, 28, 9), PULocationID=74, total_amount=9.36),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 28, 12, 11, 44), PULocationID=129, total_amount=15.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 4, 11, 38), PULocationID=25, total_amount=28.88),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 30, 10, 3, 36), PULocationID=182, total_amount=12.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 30, 20, 56, 17), PULocationID=210, total_amount=7.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e837bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 14, 13, 28, 9), PULocationID=74, total_amount=9.36)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda row:True).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0bf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa2b00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dd326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4b7006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 14, 13, 28, 9), PULocationID=74, total_amount=9.36)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d99eb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row): \n",
    "    # truncate to hour\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    # composite key\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "    \n",
    "    # tuple of composite key & composite value\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3aec7",
   "metadata": {},
   "source": [
    "invoke this calculate_revenue() to reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb328a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    # unpack the composite key\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    # unpack the composite value\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea260f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e8fc8",
   "metadata": {},
   "source": [
    "column names are lost so we define it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dae6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d791c9",
   "metadata": {},
   "source": [
    "unnested / unwrap with unwrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0a98ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a09200b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c14d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054217d",
   "metadata": {},
   "source": [
    "the final built low-level RDD code from our original SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa77c38",
   "metadata": {},
   "source": [
    "```python\n",
    "df_result = rdd \\\n",
    "    # where\n",
    "    .filter(filter_outliers)  \\\n",
    "    # select part, prep for groupBy \n",
    "    .map(prepare_for_grouping)  \\\n",
    "    # groupby and the summing + count\n",
    "    .reduceByKey(calculate_revenue)  \\\n",
    "    # unwrap from spark format\n",
    "    .map(unwrap)  \\\n",
    "    # to dataframe format\n",
    "    .toDF(result_schema) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56ea72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "  .filter(filter_outliers) \\\n",
    "  .map(prepare_for_grouping) \\\n",
    "  .reduceByKey(calculate_revenue) \\\n",
    "  .map(unwrap) \\\n",
    "  .toDF(result_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "401255f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-29 04:00:00| 116|              5.81|    1|\n",
      "|2020-01-17 22:00:00|  74|            682.38|   48|\n",
      "|2020-01-15 22:00:00|  82| 338.5600000000001|   27|\n",
      "|2020-01-21 10:00:00|  65|417.14000000000004|   22|\n",
      "|2020-01-15 01:00:00|  80|             28.79|    3|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4675bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('../../data/tmp/green-revenue', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83be98",
   "metadata": {},
   "source": [
    "## MapPartition\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    A[partition]-->|model|B(MapPartition)\n",
    "    B-->|apply model per partition| B\n",
    "    B-->|ML predictions|C[partition]\n",
    "\n",
    "```\n",
    "\n",
    "Say we want to predict `duration of a trip` with ML. We use the dependent variables in `columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "255b5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd7ee34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 14, 13, 28, 9), PULocationID=74, DOLocationID=75, trip_distance=1.35),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 28, 12, 11, 44), PULocationID=129, DOLocationID=179, trip_distance=3.73)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe1402",
   "metadata": {},
   "source": [
    "How mapPartition works\n",
    "\n",
    "- just like in Pandas.map(), it iterates over the rows\n",
    "- the return value needs to be an iterable ie a list\n",
    "- the len(return[1]) means there's that many partitions\n",
    "- actual output is `[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]`\n",
    "- and then it flattens this list (removing the nestings) to become `[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`\n",
    "- the `partition` object (being passed as args) is actually not a python list but an iterator `itertools.chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c4ee0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(partition):\n",
    "    return [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99dc6e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions(apply_model_in_batch).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "645c3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "921e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f50db3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8ecc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_distance']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "203a4e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-14 13:28:09</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-28 12:11:44</td>\n",
       "      <td>129</td>\n",
       "      <td>179</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime  PULocationID  DOLocationID  trip_distance\n",
       "0       2.0  2020-01-14 13:28:09            74            75           1.35\n",
       "1       2.0  2020-01-28 12:11:44           129           179           3.73"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6766c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no model, so it's a pretend step with an arbitrary function\n",
    "# model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d66faf",
   "metadata": {},
   "source": [
    "pd.DataFrame(rows, columns=columns) is the whole DF, might result in memory issues. but I'd changed the default to 10g in conf file, taken from `spark-defaults.conf.template`\n",
    "\n",
    "path to config file: `\\spark\\spark-3.5.1-bin-hadoop3\\conf`\n",
    "\n",
    "uncomment the `driver` line and add the `executor` line\n",
    "\n",
    "```\n",
    "spark.driver.memory              10g\n",
    "spark.executor.memory            10g\n",
    "```\n",
    "\n",
    "Example of `yield row` using infinite_seq() below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02e9dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_seq():\n",
    "    i =0\n",
    "    while True:\n",
    "        yield i\n",
    "        i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7866d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = infinite_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36386885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for i in seq:\n",
    "    print(i)\n",
    "\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7437b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b5845",
   "metadata": {},
   "source": [
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch)\\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6055d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|              6.75|\n",
      "|             18.65|\n",
      "|55.949999999999996|\n",
      "|6.6000000000000005|\n",
      "|               4.0|\n",
      "|               6.2|\n",
      "|               2.6|\n",
      "|               2.9|\n",
      "|             33.95|\n",
      "|48.449999999999996|\n",
      "|              6.25|\n",
      "|              6.15|\n",
      "|19.950000000000003|\n",
      "|2.8499999999999996|\n",
      "|               7.5|\n",
      "|              0.95|\n",
      "|              9.05|\n",
      "|              7.45|\n",
      "|              5.25|\n",
      "|             17.35|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_predicts.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91d243",
   "metadata": {},
   "source": [
    "Actually this would need to be a real-time service and for one customer at a time. \n",
    "\n",
    "Customer, the `taxi rider` input the postal code and expects the predicted duration of the trip back on their mobile\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    A[taxi rider]-->|input destination postal code|B(Taxi Service)\n",
    "    B-->|ML on mapPartition| B\n",
    "    B-->|predict duration, eg: 20mins|A\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
